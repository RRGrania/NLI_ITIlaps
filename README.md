# NLP Labs & Research Notes
Tasks of NLP Course of ITI
This repository contains practical labs and exercises in Natural Language Processing (NLP) alongside summarized notes and key insights from the "Efficient Estimation of Word Representations in Vector Space" (Word2Vec) paper.

##  Repository Structure

- `labs/`: Hands-on NLP lab notebooks and experiments.
- `paper_notes.md`: Summary and key takeaways from the "Efficient Estimation of Word Representations in Vector Space" (Word2Vec) paper.
- `requirements.txt`: List of required packages to run the labs smoothly.

## Contents

### NLP Labs
- Tokenization and Text Preprocessing
- Named Entity Recognition (NER)
- POS Tagging
- Word Embeddings (Word2Vec, GloVe)
- Text Classification
- Dependency Parsing
- Custom NER with spaCy

### üìù Research Paper Notes
A concise breakdown of the "Efficient Estimation of Word Representations in Vector Space" paper, covering:
- **Paper Title**: Efficient Estimation of Word Representations in Vector Space
- **Authors**: Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, Jeffrey Dean
- **Problem Statement**: Estimating high-quality word representations (word vectors) efficiently for NLP tasks.
- **Methodology**: The paper introduces two models (Continuous Bag of Words and Skip-gram) for generating word vectors from large corpora using neural networks.
- **Key Contributions**: Introduction of Word2Vec model for word embedding, with improved computational efficiency and performance.
- **Observations and Limitations**: Discusses advantages of Word2Vec in terms of efficiency and quality of embeddings, while noting some challenges in the optimization process.

